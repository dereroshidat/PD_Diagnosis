{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c3937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/anaconda3/envs/home/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3240e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4-bit quantization (smallest VRAM) ----\n",
    "bnb_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # bf16 compute for stability/speed\n",
    ")\n",
    "\n",
    "\n",
    "# Some envs prefer explicit dtype even when quantized\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0d5667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading quantized model… (this can take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]\n",
      "Some weights of Gemma3nModel were not initialized from the model checkpoint at google/gemma-3n-E2B and are newly initialized: ['audio_tower.conformer.0.attention.attn.k_proj.weight', 'audio_tower.conformer.0.attention.attn.per_dim_scale', 'audio_tower.conformer.0.attention.attn.q_proj.weight', 'audio_tower.conformer.0.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.0.attention.attn.v_proj.weight', 'audio_tower.conformer.0.attention.post.weight', 'audio_tower.conformer.0.attention.post_norm.weight', 'audio_tower.conformer.0.attention.pre_attn_norm.weight', 'audio_tower.conformer.0.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.0.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.0.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.0.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.0.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.0.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.0.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.0.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.0.lconv1d.conv_norm.weight', 'audio_tower.conformer.0.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.0.lconv1d.linear_end.weight', 'audio_tower.conformer.0.lconv1d.linear_start.weight', 'audio_tower.conformer.0.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.0.norm.weight', 'audio_tower.conformer.1.attention.attn.k_proj.weight', 'audio_tower.conformer.1.attention.attn.per_dim_scale', 'audio_tower.conformer.1.attention.attn.q_proj.weight', 'audio_tower.conformer.1.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.1.attention.attn.v_proj.weight', 'audio_tower.conformer.1.attention.post.weight', 'audio_tower.conformer.1.attention.post_norm.weight', 'audio_tower.conformer.1.attention.pre_attn_norm.weight', 'audio_tower.conformer.1.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.1.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.1.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.1.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.1.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.1.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.1.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.1.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.1.lconv1d.conv_norm.weight', 'audio_tower.conformer.1.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.1.lconv1d.linear_end.weight', 'audio_tower.conformer.1.lconv1d.linear_start.weight', 'audio_tower.conformer.1.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.1.norm.weight', 'audio_tower.conformer.10.attention.attn.k_proj.weight', 'audio_tower.conformer.10.attention.attn.per_dim_scale', 'audio_tower.conformer.10.attention.attn.q_proj.weight', 'audio_tower.conformer.10.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.10.attention.attn.v_proj.weight', 'audio_tower.conformer.10.attention.post.weight', 'audio_tower.conformer.10.attention.post_norm.weight', 'audio_tower.conformer.10.attention.pre_attn_norm.weight', 'audio_tower.conformer.10.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.10.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.10.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.10.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.10.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.10.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.10.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.10.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.10.lconv1d.conv_norm.weight', 'audio_tower.conformer.10.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.10.lconv1d.linear_end.weight', 'audio_tower.conformer.10.lconv1d.linear_start.weight', 'audio_tower.conformer.10.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.10.norm.weight', 'audio_tower.conformer.11.attention.attn.k_proj.weight', 'audio_tower.conformer.11.attention.attn.per_dim_scale', 'audio_tower.conformer.11.attention.attn.q_proj.weight', 'audio_tower.conformer.11.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.11.attention.attn.v_proj.weight', 'audio_tower.conformer.11.attention.post.weight', 'audio_tower.conformer.11.attention.post_norm.weight', 'audio_tower.conformer.11.attention.pre_attn_norm.weight', 'audio_tower.conformer.11.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.11.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.11.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.11.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.11.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.11.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.11.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.11.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.11.lconv1d.conv_norm.weight', 'audio_tower.conformer.11.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.11.lconv1d.linear_end.weight', 'audio_tower.conformer.11.lconv1d.linear_start.weight', 'audio_tower.conformer.11.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.11.norm.weight', 'audio_tower.conformer.2.attention.attn.k_proj.weight', 'audio_tower.conformer.2.attention.attn.per_dim_scale', 'audio_tower.conformer.2.attention.attn.q_proj.weight', 'audio_tower.conformer.2.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.2.attention.attn.v_proj.weight', 'audio_tower.conformer.2.attention.post.weight', 'audio_tower.conformer.2.attention.post_norm.weight', 'audio_tower.conformer.2.attention.pre_attn_norm.weight', 'audio_tower.conformer.2.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.2.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.2.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.2.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.2.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.2.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.2.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.2.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.2.lconv1d.conv_norm.weight', 'audio_tower.conformer.2.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.2.lconv1d.linear_end.weight', 'audio_tower.conformer.2.lconv1d.linear_start.weight', 'audio_tower.conformer.2.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.2.norm.weight', 'audio_tower.conformer.3.attention.attn.k_proj.weight', 'audio_tower.conformer.3.attention.attn.per_dim_scale', 'audio_tower.conformer.3.attention.attn.q_proj.weight', 'audio_tower.conformer.3.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.3.attention.attn.v_proj.weight', 'audio_tower.conformer.3.attention.post.weight', 'audio_tower.conformer.3.attention.post_norm.weight', 'audio_tower.conformer.3.attention.pre_attn_norm.weight', 'audio_tower.conformer.3.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.3.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.3.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.3.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.3.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.3.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.3.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.3.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.3.lconv1d.conv_norm.weight', 'audio_tower.conformer.3.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.3.lconv1d.linear_end.weight', 'audio_tower.conformer.3.lconv1d.linear_start.weight', 'audio_tower.conformer.3.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.3.norm.weight', 'audio_tower.conformer.4.attention.attn.k_proj.weight', 'audio_tower.conformer.4.attention.attn.per_dim_scale', 'audio_tower.conformer.4.attention.attn.q_proj.weight', 'audio_tower.conformer.4.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.4.attention.attn.v_proj.weight', 'audio_tower.conformer.4.attention.post.weight', 'audio_tower.conformer.4.attention.post_norm.weight', 'audio_tower.conformer.4.attention.pre_attn_norm.weight', 'audio_tower.conformer.4.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.4.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.4.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.4.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.4.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.4.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.4.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.4.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.4.lconv1d.conv_norm.weight', 'audio_tower.conformer.4.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.4.lconv1d.linear_end.weight', 'audio_tower.conformer.4.lconv1d.linear_start.weight', 'audio_tower.conformer.4.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.4.norm.weight', 'audio_tower.conformer.5.attention.attn.k_proj.weight', 'audio_tower.conformer.5.attention.attn.per_dim_scale', 'audio_tower.conformer.5.attention.attn.q_proj.weight', 'audio_tower.conformer.5.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.5.attention.attn.v_proj.weight', 'audio_tower.conformer.5.attention.post.weight', 'audio_tower.conformer.5.attention.post_norm.weight', 'audio_tower.conformer.5.attention.pre_attn_norm.weight', 'audio_tower.conformer.5.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.5.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.5.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.5.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.5.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.5.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.5.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.5.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.5.lconv1d.conv_norm.weight', 'audio_tower.conformer.5.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.5.lconv1d.linear_end.weight', 'audio_tower.conformer.5.lconv1d.linear_start.weight', 'audio_tower.conformer.5.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.5.norm.weight', 'audio_tower.conformer.6.attention.attn.k_proj.weight', 'audio_tower.conformer.6.attention.attn.per_dim_scale', 'audio_tower.conformer.6.attention.attn.q_proj.weight', 'audio_tower.conformer.6.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.6.attention.attn.v_proj.weight', 'audio_tower.conformer.6.attention.post.weight', 'audio_tower.conformer.6.attention.post_norm.weight', 'audio_tower.conformer.6.attention.pre_attn_norm.weight', 'audio_tower.conformer.6.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.6.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.6.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.6.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.6.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.6.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.6.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.6.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.6.lconv1d.conv_norm.weight', 'audio_tower.conformer.6.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.6.lconv1d.linear_end.weight', 'audio_tower.conformer.6.lconv1d.linear_start.weight', 'audio_tower.conformer.6.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.6.norm.weight', 'audio_tower.conformer.7.attention.attn.k_proj.weight', 'audio_tower.conformer.7.attention.attn.per_dim_scale', 'audio_tower.conformer.7.attention.attn.q_proj.weight', 'audio_tower.conformer.7.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.7.attention.attn.v_proj.weight', 'audio_tower.conformer.7.attention.post.weight', 'audio_tower.conformer.7.attention.post_norm.weight', 'audio_tower.conformer.7.attention.pre_attn_norm.weight', 'audio_tower.conformer.7.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.7.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.7.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.7.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.7.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.7.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.7.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.7.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.7.lconv1d.conv_norm.weight', 'audio_tower.conformer.7.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.7.lconv1d.linear_end.weight', 'audio_tower.conformer.7.lconv1d.linear_start.weight', 'audio_tower.conformer.7.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.7.norm.weight', 'audio_tower.conformer.8.attention.attn.k_proj.weight', 'audio_tower.conformer.8.attention.attn.per_dim_scale', 'audio_tower.conformer.8.attention.attn.q_proj.weight', 'audio_tower.conformer.8.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.8.attention.attn.v_proj.weight', 'audio_tower.conformer.8.attention.post.weight', 'audio_tower.conformer.8.attention.post_norm.weight', 'audio_tower.conformer.8.attention.pre_attn_norm.weight', 'audio_tower.conformer.8.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.8.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.8.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.8.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.8.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.8.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.8.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.8.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.8.lconv1d.conv_norm.weight', 'audio_tower.conformer.8.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.8.lconv1d.linear_end.weight', 'audio_tower.conformer.8.lconv1d.linear_start.weight', 'audio_tower.conformer.8.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.8.norm.weight', 'audio_tower.conformer.9.attention.attn.k_proj.weight', 'audio_tower.conformer.9.attention.attn.per_dim_scale', 'audio_tower.conformer.9.attention.attn.q_proj.weight', 'audio_tower.conformer.9.attention.attn.relative_position_embedding.pos_proj.weight', 'audio_tower.conformer.9.attention.attn.v_proj.weight', 'audio_tower.conformer.9.attention.post.weight', 'audio_tower.conformer.9.attention.post_norm.weight', 'audio_tower.conformer.9.attention.pre_attn_norm.weight', 'audio_tower.conformer.9.ffw_layer_end.ffw_layer_1.weight', 'audio_tower.conformer.9.ffw_layer_end.ffw_layer_2.weight', 'audio_tower.conformer.9.ffw_layer_end.post_layer_norm.weight', 'audio_tower.conformer.9.ffw_layer_end.pre_layer_norm.weight', 'audio_tower.conformer.9.ffw_layer_start.ffw_layer_1.weight', 'audio_tower.conformer.9.ffw_layer_start.ffw_layer_2.weight', 'audio_tower.conformer.9.ffw_layer_start.post_layer_norm.weight', 'audio_tower.conformer.9.ffw_layer_start.pre_layer_norm.weight', 'audio_tower.conformer.9.lconv1d.conv_norm.weight', 'audio_tower.conformer.9.lconv1d.depthwise_conv1d.weight', 'audio_tower.conformer.9.lconv1d.linear_end.weight', 'audio_tower.conformer.9.lconv1d.linear_start.weight', 'audio_tower.conformer.9.lconv1d.pre_layer_norm.weight', 'audio_tower.conformer.9.norm.weight', 'audio_tower.subsample_conv_projection.conv_0.conv.weight', 'audio_tower.subsample_conv_projection.conv_0.norm.weight', 'audio_tower.subsample_conv_projection.conv_1.conv.weight', 'audio_tower.subsample_conv_projection.conv_1.norm.weight', 'audio_tower.subsample_conv_projection.input_proj_linear.weight', 'embed_audio.embedding.weight', 'embed_audio.embedding_projection.weight', 'embed_audio.hard_embedding_norm.weight', 'embed_audio.soft_embedding_norm.weight', 'embed_vision.embedding.weight', 'embed_vision.embedding_projection.weight', 'embed_vision.hard_embedding_norm.weight', 'embed_vision.soft_embedding_norm.weight', 'language_model.altup_projections.0.weight', 'language_model.altup_projections.1.weight', 'language_model.altup_projections.2.weight', 'language_model.altup_unembed_projections.0.weight', 'language_model.altup_unembed_projections.1.weight', 'language_model.altup_unembed_projections.2.weight', 'language_model.embed_tokens.weight', 'language_model.embed_tokens_per_layer.weight', 'language_model.layers.0.altup.correct_output_scale', 'language_model.layers.0.altup.correction_coefs.weight', 'language_model.layers.0.altup.modality_router.weight', 'language_model.layers.0.altup.prediction_coefs.weight', 'language_model.layers.0.altup.router_norm.weight', 'language_model.layers.0.input_layernorm.weight', 'language_model.layers.0.laurel.linear_left.weight', 'language_model.layers.0.laurel.linear_right.weight', 'language_model.layers.0.laurel.post_laurel_norm.weight', 'language_model.layers.0.mlp.down_proj.weight', 'language_model.layers.0.mlp.gate_proj.weight', 'language_model.layers.0.mlp.up_proj.weight', 'language_model.layers.0.per_layer_input_gate.weight', 'language_model.layers.0.per_layer_projection.weight', 'language_model.layers.0.post_attention_layernorm.weight', 'language_model.layers.0.post_feedforward_layernorm.weight', 'language_model.layers.0.post_per_layer_input_norm.weight', 'language_model.layers.0.pre_feedforward_layernorm.weight', 'language_model.layers.0.self_attn.k_norm.weight', 'language_model.layers.0.self_attn.k_proj.weight', 'language_model.layers.0.self_attn.o_proj.weight', 'language_model.layers.0.self_attn.q_norm.weight', 'language_model.layers.0.self_attn.q_proj.weight', 'language_model.layers.0.self_attn.v_proj.weight', 'language_model.layers.1.altup.correct_output_scale', 'language_model.layers.1.altup.correction_coefs.weight', 'language_model.layers.1.altup.modality_router.weight', 'language_model.layers.1.altup.prediction_coefs.weight', 'language_model.layers.1.altup.router_norm.weight', 'language_model.layers.1.input_layernorm.weight', 'language_model.layers.1.laurel.linear_left.weight', 'language_model.layers.1.laurel.linear_right.weight', 'language_model.layers.1.laurel.post_laurel_norm.weight', 'language_model.layers.1.mlp.down_proj.weight', 'language_model.layers.1.mlp.gate_proj.weight', 'language_model.layers.1.mlp.up_proj.weight', 'language_model.layers.1.per_layer_input_gate.weight', 'language_model.layers.1.per_layer_projection.weight', 'language_model.layers.1.post_attention_layernorm.weight', 'language_model.layers.1.post_feedforward_layernorm.weight', 'language_model.layers.1.post_per_layer_input_norm.weight', 'language_model.layers.1.pre_feedforward_layernorm.weight', 'language_model.layers.1.self_attn.k_norm.weight', 'language_model.layers.1.self_attn.k_proj.weight', 'language_model.layers.1.self_attn.o_proj.weight', 'language_model.layers.1.self_attn.q_norm.weight', 'language_model.layers.1.self_attn.q_proj.weight', 'language_model.layers.1.self_attn.v_proj.weight', 'language_model.layers.10.altup.correct_output_scale', 'language_model.layers.10.altup.correction_coefs.weight', 'language_model.layers.10.altup.modality_router.weight', 'language_model.layers.10.altup.prediction_coefs.weight', 'language_model.layers.10.altup.router_norm.weight', 'language_model.layers.10.input_layernorm.weight', 'language_model.layers.10.laurel.linear_left.weight', 'language_model.layers.10.laurel.linear_right.weight', 'language_model.layers.10.laurel.post_laurel_norm.weight', 'language_model.layers.10.mlp.down_proj.weight', 'language_model.layers.10.mlp.gate_proj.weight', 'language_model.layers.10.mlp.up_proj.weight', 'language_model.layers.10.per_layer_input_gate.weight', 'language_model.layers.10.per_layer_projection.weight', 'language_model.layers.10.post_attention_layernorm.weight', 'language_model.layers.10.post_feedforward_layernorm.weight', 'language_model.layers.10.post_per_layer_input_norm.weight', 'language_model.layers.10.pre_feedforward_layernorm.weight', 'language_model.layers.10.self_attn.k_norm.weight', 'language_model.layers.10.self_attn.k_proj.weight', 'language_model.layers.10.self_attn.o_proj.weight', 'language_model.layers.10.self_attn.q_norm.weight', 'language_model.layers.10.self_attn.q_proj.weight', 'language_model.layers.10.self_attn.v_proj.weight', 'language_model.layers.11.altup.correct_output_scale', 'language_model.layers.11.altup.correction_coefs.weight', 'language_model.layers.11.altup.modality_router.weight', 'language_model.layers.11.altup.prediction_coefs.weight', 'language_model.layers.11.altup.router_norm.weight', 'language_model.layers.11.input_layernorm.weight', 'language_model.layers.11.laurel.linear_left.weight', 'language_model.layers.11.laurel.linear_right.weight', 'language_model.layers.11.laurel.post_laurel_norm.weight', 'language_model.layers.11.mlp.down_proj.weight', 'language_model.layers.11.mlp.gate_proj.weight', 'language_model.layers.11.mlp.up_proj.weight', 'language_model.layers.11.per_layer_input_gate.weight', 'language_model.layers.11.per_layer_projection.weight', 'language_model.layers.11.post_attention_layernorm.weight', 'language_model.layers.11.post_feedforward_layernorm.weight', 'language_model.layers.11.post_per_layer_input_norm.weight', 'language_model.layers.11.pre_feedforward_layernorm.weight', 'language_model.layers.11.self_attn.k_norm.weight', 'language_model.layers.11.self_attn.k_proj.weight', 'language_model.layers.11.self_attn.o_proj.weight', 'language_model.layers.11.self_attn.q_norm.weight', 'language_model.layers.11.self_attn.q_proj.weight', 'language_model.layers.11.self_attn.v_proj.weight', 'language_model.layers.12.altup.correct_output_scale', 'language_model.layers.12.altup.correction_coefs.weight', 'language_model.layers.12.altup.modality_router.weight', 'language_model.layers.12.altup.prediction_coefs.weight', 'language_model.layers.12.altup.router_norm.weight', 'language_model.layers.12.input_layernorm.weight', 'language_model.layers.12.laurel.linear_left.weight', 'language_model.layers.12.laurel.linear_right.weight', 'language_model.layers.12.laurel.post_laurel_norm.weight', 'language_model.layers.12.mlp.down_proj.weight', 'language_model.layers.12.mlp.gate_proj.weight', 'language_model.layers.12.mlp.up_proj.weight', 'language_model.layers.12.per_layer_input_gate.weight', 'language_model.layers.12.per_layer_projection.weight', 'language_model.layers.12.post_attention_layernorm.weight', 'language_model.layers.12.post_feedforward_layernorm.weight', 'language_model.layers.12.post_per_layer_input_norm.weight', 'language_model.layers.12.pre_feedforward_layernorm.weight', 'language_model.layers.12.self_attn.k_norm.weight', 'language_model.layers.12.self_attn.k_proj.weight', 'language_model.layers.12.self_attn.o_proj.weight', 'language_model.layers.12.self_attn.q_norm.weight', 'language_model.layers.12.self_attn.q_proj.weight', 'language_model.layers.12.self_attn.v_proj.weight', 'language_model.layers.13.altup.correct_output_scale', 'language_model.layers.13.altup.correction_coefs.weight', 'language_model.layers.13.altup.modality_router.weight', 'language_model.layers.13.altup.prediction_coefs.weight', 'language_model.layers.13.altup.router_norm.weight', 'language_model.layers.13.input_layernorm.weight', 'language_model.layers.13.laurel.linear_left.weight', 'language_model.layers.13.laurel.linear_right.weight', 'language_model.layers.13.laurel.post_laurel_norm.weight', 'language_model.layers.13.mlp.down_proj.weight', 'language_model.layers.13.mlp.gate_proj.weight', 'language_model.layers.13.mlp.up_proj.weight', 'language_model.layers.13.per_layer_input_gate.weight', 'language_model.layers.13.per_layer_projection.weight', 'language_model.layers.13.post_attention_layernorm.weight', 'language_model.layers.13.post_feedforward_layernorm.weight', 'language_model.layers.13.post_per_layer_input_norm.weight', 'language_model.layers.13.pre_feedforward_layernorm.weight', 'language_model.layers.13.self_attn.k_norm.weight', 'language_model.layers.13.self_attn.k_proj.weight', 'language_model.layers.13.self_attn.o_proj.weight', 'language_model.layers.13.self_attn.q_norm.weight', 'language_model.layers.13.self_attn.q_proj.weight', 'language_model.layers.13.self_attn.v_proj.weight', 'language_model.layers.14.altup.correct_output_scale', 'language_model.layers.14.altup.correction_coefs.weight', 'language_model.layers.14.altup.modality_router.weight', 'language_model.layers.14.altup.prediction_coefs.weight', 'language_model.layers.14.altup.router_norm.weight', 'language_model.layers.14.input_layernorm.weight', 'language_model.layers.14.laurel.linear_left.weight', 'language_model.layers.14.laurel.linear_right.weight', 'language_model.layers.14.laurel.post_laurel_norm.weight', 'language_model.layers.14.mlp.down_proj.weight', 'language_model.layers.14.mlp.gate_proj.weight', 'language_model.layers.14.mlp.up_proj.weight', 'language_model.layers.14.per_layer_input_gate.weight', 'language_model.layers.14.per_layer_projection.weight', 'language_model.layers.14.post_attention_layernorm.weight', 'language_model.layers.14.post_feedforward_layernorm.weight', 'language_model.layers.14.post_per_layer_input_norm.weight', 'language_model.layers.14.pre_feedforward_layernorm.weight', 'language_model.layers.14.self_attn.k_norm.weight', 'language_model.layers.14.self_attn.k_proj.weight', 'language_model.layers.14.self_attn.o_proj.weight', 'language_model.layers.14.self_attn.q_norm.weight', 'language_model.layers.14.self_attn.q_proj.weight', 'language_model.layers.14.self_attn.v_proj.weight', 'language_model.layers.15.altup.correct_output_scale', 'language_model.layers.15.altup.correction_coefs.weight', 'language_model.layers.15.altup.modality_router.weight', 'language_model.layers.15.altup.prediction_coefs.weight', 'language_model.layers.15.altup.router_norm.weight', 'language_model.layers.15.input_layernorm.weight', 'language_model.layers.15.laurel.linear_left.weight', 'language_model.layers.15.laurel.linear_right.weight', 'language_model.layers.15.laurel.post_laurel_norm.weight', 'language_model.layers.15.mlp.down_proj.weight', 'language_model.layers.15.mlp.gate_proj.weight', 'language_model.layers.15.mlp.up_proj.weight', 'language_model.layers.15.per_layer_input_gate.weight', 'language_model.layers.15.per_layer_projection.weight', 'language_model.layers.15.post_attention_layernorm.weight', 'language_model.layers.15.post_feedforward_layernorm.weight', 'language_model.layers.15.post_per_layer_input_norm.weight', 'language_model.layers.15.pre_feedforward_layernorm.weight', 'language_model.layers.15.self_attn.k_norm.weight', 'language_model.layers.15.self_attn.k_proj.weight', 'language_model.layers.15.self_attn.o_proj.weight', 'language_model.layers.15.self_attn.q_norm.weight', 'language_model.layers.15.self_attn.q_proj.weight', 'language_model.layers.15.self_attn.v_proj.weight', 'language_model.layers.16.altup.correct_output_scale', 'language_model.layers.16.altup.correction_coefs.weight', 'language_model.layers.16.altup.modality_router.weight', 'language_model.layers.16.altup.prediction_coefs.weight', 'language_model.layers.16.altup.router_norm.weight', 'language_model.layers.16.input_layernorm.weight', 'language_model.layers.16.laurel.linear_left.weight', 'language_model.layers.16.laurel.linear_right.weight', 'language_model.layers.16.laurel.post_laurel_norm.weight', 'language_model.layers.16.mlp.down_proj.weight', 'language_model.layers.16.mlp.gate_proj.weight', 'language_model.layers.16.mlp.up_proj.weight', 'language_model.layers.16.per_layer_input_gate.weight', 'language_model.layers.16.per_layer_projection.weight', 'language_model.layers.16.post_attention_layernorm.weight', 'language_model.layers.16.post_feedforward_layernorm.weight', 'language_model.layers.16.post_per_layer_input_norm.weight', 'language_model.layers.16.pre_feedforward_layernorm.weight', 'language_model.layers.16.self_attn.k_norm.weight', 'language_model.layers.16.self_attn.k_proj.weight', 'language_model.layers.16.self_attn.o_proj.weight', 'language_model.layers.16.self_attn.q_norm.weight', 'language_model.layers.16.self_attn.q_proj.weight', 'language_model.layers.16.self_attn.v_proj.weight', 'language_model.layers.17.altup.correct_output_scale', 'language_model.layers.17.altup.correction_coefs.weight', 'language_model.layers.17.altup.modality_router.weight', 'language_model.layers.17.altup.prediction_coefs.weight', 'language_model.layers.17.altup.router_norm.weight', 'language_model.layers.17.input_layernorm.weight', 'language_model.layers.17.laurel.linear_left.weight', 'language_model.layers.17.laurel.linear_right.weight', 'language_model.layers.17.laurel.post_laurel_norm.weight', 'language_model.layers.17.mlp.down_proj.weight', 'language_model.layers.17.mlp.gate_proj.weight', 'language_model.layers.17.mlp.up_proj.weight', 'language_model.layers.17.per_layer_input_gate.weight', 'language_model.layers.17.per_layer_projection.weight', 'language_model.layers.17.post_attention_layernorm.weight', 'language_model.layers.17.post_feedforward_layernorm.weight', 'language_model.layers.17.post_per_layer_input_norm.weight', 'language_model.layers.17.pre_feedforward_layernorm.weight', 'language_model.layers.17.self_attn.k_norm.weight', 'language_model.layers.17.self_attn.k_proj.weight', 'language_model.layers.17.self_attn.o_proj.weight', 'language_model.layers.17.self_attn.q_norm.weight', 'language_model.layers.17.self_attn.q_proj.weight', 'language_model.layers.17.self_attn.v_proj.weight', 'language_model.layers.18.altup.correct_output_scale', 'language_model.layers.18.altup.correction_coefs.weight', 'language_model.layers.18.altup.modality_router.weight', 'language_model.layers.18.altup.prediction_coefs.weight', 'language_model.layers.18.altup.router_norm.weight', 'language_model.layers.18.input_layernorm.weight', 'language_model.layers.18.laurel.linear_left.weight', 'language_model.layers.18.laurel.linear_right.weight', 'language_model.layers.18.laurel.post_laurel_norm.weight', 'language_model.layers.18.mlp.down_proj.weight', 'language_model.layers.18.mlp.gate_proj.weight', 'language_model.layers.18.mlp.up_proj.weight', 'language_model.layers.18.per_layer_input_gate.weight', 'language_model.layers.18.per_layer_projection.weight', 'language_model.layers.18.post_attention_layernorm.weight', 'language_model.layers.18.post_feedforward_layernorm.weight', 'language_model.layers.18.post_per_layer_input_norm.weight', 'language_model.layers.18.pre_feedforward_layernorm.weight', 'language_model.layers.18.self_attn.k_norm.weight', 'language_model.layers.18.self_attn.k_proj.weight', 'language_model.layers.18.self_attn.o_proj.weight', 'language_model.layers.18.self_attn.q_norm.weight', 'language_model.layers.18.self_attn.q_proj.weight', 'language_model.layers.18.self_attn.v_proj.weight', 'language_model.layers.19.altup.correct_output_scale', 'language_model.layers.19.altup.correction_coefs.weight', 'language_model.layers.19.altup.modality_router.weight', 'language_model.layers.19.altup.prediction_coefs.weight', 'language_model.layers.19.altup.router_norm.weight', 'language_model.layers.19.input_layernorm.weight', 'language_model.layers.19.laurel.linear_left.weight', 'language_model.layers.19.laurel.linear_right.weight', 'language_model.layers.19.laurel.post_laurel_norm.weight', 'language_model.layers.19.mlp.down_proj.weight', 'language_model.layers.19.mlp.gate_proj.weight', 'language_model.layers.19.mlp.up_proj.weight', 'language_model.layers.19.per_layer_input_gate.weight', 'language_model.layers.19.per_layer_projection.weight', 'language_model.layers.19.post_attention_layernorm.weight', 'language_model.layers.19.post_feedforward_layernorm.weight', 'language_model.layers.19.post_per_layer_input_norm.weight', 'language_model.layers.19.pre_feedforward_layernorm.weight', 'language_model.layers.19.self_attn.k_norm.weight', 'language_model.layers.19.self_attn.k_proj.weight', 'language_model.layers.19.self_attn.o_proj.weight', 'language_model.layers.19.self_attn.q_norm.weight', 'language_model.layers.19.self_attn.q_proj.weight', 'language_model.layers.19.self_attn.v_proj.weight', 'language_model.layers.2.altup.correct_output_scale', 'language_model.layers.2.altup.correction_coefs.weight', 'language_model.layers.2.altup.modality_router.weight', 'language_model.layers.2.altup.prediction_coefs.weight', 'language_model.layers.2.altup.router_norm.weight', 'language_model.layers.2.input_layernorm.weight', 'language_model.layers.2.laurel.linear_left.weight', 'language_model.layers.2.laurel.linear_right.weight', 'language_model.layers.2.laurel.post_laurel_norm.weight', 'language_model.layers.2.mlp.down_proj.weight', 'language_model.layers.2.mlp.gate_proj.weight', 'language_model.layers.2.mlp.up_proj.weight', 'language_model.layers.2.per_layer_input_gate.weight', 'language_model.layers.2.per_layer_projection.weight', 'language_model.layers.2.post_attention_layernorm.weight', 'language_model.layers.2.post_feedforward_layernorm.weight', 'language_model.layers.2.post_per_layer_input_norm.weight', 'language_model.layers.2.pre_feedforward_layernorm.weight', 'language_model.layers.2.self_attn.k_norm.weight', 'language_model.layers.2.self_attn.k_proj.weight', 'language_model.layers.2.self_attn.o_proj.weight', 'language_model.layers.2.self_attn.q_norm.weight', 'language_model.layers.2.self_attn.q_proj.weight', 'language_model.layers.2.self_attn.v_proj.weight', 'language_model.layers.20.altup.correct_output_scale', 'language_model.layers.20.altup.correction_coefs.weight', 'language_model.layers.20.altup.modality_router.weight', 'language_model.layers.20.altup.prediction_coefs.weight', 'language_model.layers.20.altup.router_norm.weight', 'language_model.layers.20.input_layernorm.weight', 'language_model.layers.20.laurel.linear_left.weight', 'language_model.layers.20.laurel.linear_right.weight', 'language_model.layers.20.laurel.post_laurel_norm.weight', 'language_model.layers.20.mlp.down_proj.weight', 'language_model.layers.20.mlp.gate_proj.weight', 'language_model.layers.20.mlp.up_proj.weight', 'language_model.layers.20.per_layer_input_gate.weight', 'language_model.layers.20.per_layer_projection.weight', 'language_model.layers.20.post_attention_layernorm.weight', 'language_model.layers.20.post_feedforward_layernorm.weight', 'language_model.layers.20.post_per_layer_input_norm.weight', 'language_model.layers.20.pre_feedforward_layernorm.weight', 'language_model.layers.20.self_attn.k_norm.weight', 'language_model.layers.20.self_attn.k_proj.weight', 'language_model.layers.20.self_attn.o_proj.weight', 'language_model.layers.20.self_attn.q_norm.weight', 'language_model.layers.20.self_attn.q_proj.weight', 'language_model.layers.20.self_attn.v_proj.weight', 'language_model.layers.21.altup.correct_output_scale', 'language_model.layers.21.altup.correction_coefs.weight', 'language_model.layers.21.altup.modality_router.weight', 'language_model.layers.21.altup.prediction_coefs.weight', 'language_model.layers.21.altup.router_norm.weight', 'language_model.layers.21.input_layernorm.weight', 'language_model.layers.21.laurel.linear_left.weight', 'language_model.layers.21.laurel.linear_right.weight', 'language_model.layers.21.laurel.post_laurel_norm.weight', 'language_model.layers.21.mlp.down_proj.weight', 'language_model.layers.21.mlp.gate_proj.weight', 'language_model.layers.21.mlp.up_proj.weight', 'language_model.layers.21.per_layer_input_gate.weight', 'language_model.layers.21.per_layer_projection.weight', 'language_model.layers.21.post_attention_layernorm.weight', 'language_model.layers.21.post_feedforward_layernorm.weight', 'language_model.layers.21.post_per_layer_input_norm.weight', 'language_model.layers.21.pre_feedforward_layernorm.weight', 'language_model.layers.21.self_attn.k_norm.weight', 'language_model.layers.21.self_attn.k_proj.weight', 'language_model.layers.21.self_attn.o_proj.weight', 'language_model.layers.21.self_attn.q_norm.weight', 'language_model.layers.21.self_attn.q_proj.weight', 'language_model.layers.21.self_attn.v_proj.weight', 'language_model.layers.22.altup.correct_output_scale', 'language_model.layers.22.altup.correction_coefs.weight', 'language_model.layers.22.altup.modality_router.weight', 'language_model.layers.22.altup.prediction_coefs.weight', 'language_model.layers.22.altup.router_norm.weight', 'language_model.layers.22.input_layernorm.weight', 'language_model.layers.22.laurel.linear_left.weight', 'language_model.layers.22.laurel.linear_right.weight', 'language_model.layers.22.laurel.post_laurel_norm.weight', 'language_model.layers.22.mlp.down_proj.weight', 'language_model.layers.22.mlp.gate_proj.weight', 'language_model.layers.22.mlp.up_proj.weight', 'language_model.layers.22.per_layer_input_gate.weight', 'language_model.layers.22.per_layer_projection.weight', 'language_model.layers.22.post_attention_layernorm.weight', 'language_model.layers.22.post_feedforward_layernorm.weight', 'language_model.layers.22.post_per_layer_input_norm.weight', 'language_model.layers.22.pre_feedforward_layernorm.weight', 'language_model.layers.22.self_attn.k_norm.weight', 'language_model.layers.22.self_attn.k_proj.weight', 'language_model.layers.22.self_attn.o_proj.weight', 'language_model.layers.22.self_attn.q_norm.weight', 'language_model.layers.22.self_attn.q_proj.weight', 'language_model.layers.22.self_attn.v_proj.weight', 'language_model.layers.23.altup.correct_output_scale', 'language_model.layers.23.altup.correction_coefs.weight', 'language_model.layers.23.altup.modality_router.weight', 'language_model.layers.23.altup.prediction_coefs.weight', 'language_model.layers.23.altup.router_norm.weight', 'language_model.layers.23.input_layernorm.weight', 'language_model.layers.23.laurel.linear_left.weight', 'language_model.layers.23.laurel.linear_right.weight', 'language_model.layers.23.laurel.post_laurel_norm.weight', 'language_model.layers.23.mlp.down_proj.weight', 'language_model.layers.23.mlp.gate_proj.weight', 'language_model.layers.23.mlp.up_proj.weight', 'language_model.layers.23.per_layer_input_gate.weight', 'language_model.layers.23.per_layer_projection.weight', 'language_model.layers.23.post_attention_layernorm.weight', 'language_model.layers.23.post_feedforward_layernorm.weight', 'language_model.layers.23.post_per_layer_input_norm.weight', 'language_model.layers.23.pre_feedforward_layernorm.weight', 'language_model.layers.23.self_attn.k_norm.weight', 'language_model.layers.23.self_attn.k_proj.weight', 'language_model.layers.23.self_attn.o_proj.weight', 'language_model.layers.23.self_attn.q_norm.weight', 'language_model.layers.23.self_attn.q_proj.weight', 'language_model.layers.23.self_attn.v_proj.weight', 'language_model.layers.24.altup.correct_output_scale', 'language_model.layers.24.altup.correction_coefs.weight', 'language_model.layers.24.altup.modality_router.weight', 'language_model.layers.24.altup.prediction_coefs.weight', 'language_model.layers.24.altup.router_norm.weight', 'language_model.layers.24.input_layernorm.weight', 'language_model.layers.24.laurel.linear_left.weight', 'language_model.layers.24.laurel.linear_right.weight', 'language_model.layers.24.laurel.post_laurel_norm.weight', 'language_model.layers.24.mlp.down_proj.weight', 'language_model.layers.24.mlp.gate_proj.weight', 'language_model.layers.24.mlp.up_proj.weight', 'language_model.layers.24.per_layer_input_gate.weight', 'language_model.layers.24.per_layer_projection.weight', 'language_model.layers.24.post_attention_layernorm.weight', 'language_model.layers.24.post_feedforward_layernorm.weight', 'language_model.layers.24.post_per_layer_input_norm.weight', 'language_model.layers.24.pre_feedforward_layernorm.weight', 'language_model.layers.24.self_attn.k_norm.weight', 'language_model.layers.24.self_attn.k_proj.weight', 'language_model.layers.24.self_attn.o_proj.weight', 'language_model.layers.24.self_attn.q_norm.weight', 'language_model.layers.24.self_attn.q_proj.weight', 'language_model.layers.24.self_attn.v_proj.weight', 'language_model.layers.25.altup.correct_output_scale', 'language_model.layers.25.altup.correction_coefs.weight', 'language_model.layers.25.altup.modality_router.weight', 'language_model.layers.25.altup.prediction_coefs.weight', 'language_model.layers.25.altup.router_norm.weight', 'language_model.layers.25.input_layernorm.weight', 'language_model.layers.25.laurel.linear_left.weight', 'language_model.layers.25.laurel.linear_right.weight', 'language_model.layers.25.laurel.post_laurel_norm.weight', 'language_model.layers.25.mlp.down_proj.weight', 'language_model.layers.25.mlp.gate_proj.weight', 'language_model.layers.25.mlp.up_proj.weight', 'language_model.layers.25.per_layer_input_gate.weight', 'language_model.layers.25.per_layer_projection.weight', 'language_model.layers.25.post_attention_layernorm.weight', 'language_model.layers.25.post_feedforward_layernorm.weight', 'language_model.layers.25.post_per_layer_input_norm.weight', 'language_model.layers.25.pre_feedforward_layernorm.weight', 'language_model.layers.25.self_attn.k_norm.weight', 'language_model.layers.25.self_attn.k_proj.weight', 'language_model.layers.25.self_attn.o_proj.weight', 'language_model.layers.25.self_attn.q_norm.weight', 'language_model.layers.25.self_attn.q_proj.weight', 'language_model.layers.25.self_attn.v_proj.weight', 'language_model.layers.26.altup.correct_output_scale', 'language_model.layers.26.altup.correction_coefs.weight', 'language_model.layers.26.altup.modality_router.weight', 'language_model.layers.26.altup.prediction_coefs.weight', 'language_model.layers.26.altup.router_norm.weight', 'language_model.layers.26.input_layernorm.weight', 'language_model.layers.26.laurel.linear_left.weight', 'language_model.layers.26.laurel.linear_right.weight', 'language_model.layers.26.laurel.post_laurel_norm.weight', 'language_model.layers.26.mlp.down_proj.weight', 'language_model.layers.26.mlp.gate_proj.weight', 'language_model.layers.26.mlp.up_proj.weight', 'language_model.layers.26.per_layer_input_gate.weight', 'language_model.layers.26.per_layer_projection.weight', 'language_model.layers.26.post_attention_layernorm.weight', 'language_model.layers.26.post_feedforward_layernorm.weight', 'language_model.layers.26.post_per_layer_input_norm.weight', 'language_model.layers.26.pre_feedforward_layernorm.weight', 'language_model.layers.26.self_attn.k_norm.weight', 'language_model.layers.26.self_attn.k_proj.weight', 'language_model.layers.26.self_attn.o_proj.weight', 'language_model.layers.26.self_attn.q_norm.weight', 'language_model.layers.26.self_attn.q_proj.weight', 'language_model.layers.26.self_attn.v_proj.weight', 'language_model.layers.27.altup.correct_output_scale', 'language_model.layers.27.altup.correction_coefs.weight', 'language_model.layers.27.altup.modality_router.weight', 'language_model.layers.27.altup.prediction_coefs.weight', 'language_model.layers.27.altup.router_norm.weight', 'language_model.layers.27.input_layernorm.weight', 'language_model.layers.27.laurel.linear_left.weight', 'language_model.layers.27.laurel.linear_right.weight', 'language_model.layers.27.laurel.post_laurel_norm.weight', 'language_model.layers.27.mlp.down_proj.weight', 'language_model.layers.27.mlp.gate_proj.weight', 'language_model.layers.27.mlp.up_proj.weight', 'language_model.layers.27.per_layer_input_gate.weight', 'language_model.layers.27.per_layer_projection.weight', 'language_model.layers.27.post_attention_layernorm.weight', 'language_model.layers.27.post_feedforward_layernorm.weight', 'language_model.layers.27.post_per_layer_input_norm.weight', 'language_model.layers.27.pre_feedforward_layernorm.weight', 'language_model.layers.27.self_attn.k_norm.weight', 'language_model.layers.27.self_attn.k_proj.weight', 'language_model.layers.27.self_attn.o_proj.weight', 'language_model.layers.27.self_attn.q_norm.weight', 'language_model.layers.27.self_attn.q_proj.weight', 'language_model.layers.27.self_attn.v_proj.weight', 'language_model.layers.28.altup.correct_output_scale', 'language_model.layers.28.altup.correction_coefs.weight', 'language_model.layers.28.altup.modality_router.weight', 'language_model.layers.28.altup.prediction_coefs.weight', 'language_model.layers.28.altup.router_norm.weight', 'language_model.layers.28.input_layernorm.weight', 'language_model.layers.28.laurel.linear_left.weight', 'language_model.layers.28.laurel.linear_right.weight', 'language_model.layers.28.laurel.post_laurel_norm.weight', 'language_model.layers.28.mlp.down_proj.weight', 'language_model.layers.28.mlp.gate_proj.weight', 'language_model.layers.28.mlp.up_proj.weight', 'language_model.layers.28.per_layer_input_gate.weight', 'language_model.layers.28.per_layer_projection.weight', 'language_model.layers.28.post_attention_layernorm.weight', 'language_model.layers.28.post_feedforward_layernorm.weight', 'language_model.layers.28.post_per_layer_input_norm.weight', 'language_model.layers.28.pre_feedforward_layernorm.weight', 'language_model.layers.28.self_attn.k_norm.weight', 'language_model.layers.28.self_attn.k_proj.weight', 'language_model.layers.28.self_attn.o_proj.weight', 'language_model.layers.28.self_attn.q_norm.weight', 'language_model.layers.28.self_attn.q_proj.weight', 'language_model.layers.28.self_attn.v_proj.weight', 'language_model.layers.29.altup.correct_output_scale', 'language_model.layers.29.altup.correction_coefs.weight', 'language_model.layers.29.altup.modality_router.weight', 'language_model.layers.29.altup.prediction_coefs.weight', 'language_model.layers.29.altup.router_norm.weight', 'language_model.layers.29.input_layernorm.weight', 'language_model.layers.29.laurel.linear_left.weight', 'language_model.layers.29.laurel.linear_right.weight', 'language_model.layers.29.laurel.post_laurel_norm.weight', 'language_model.layers.29.mlp.down_proj.weight', 'language_model.layers.29.mlp.gate_proj.weight', 'language_model.layers.29.mlp.up_proj.weight', 'language_model.layers.29.per_layer_input_gate.weight', 'language_model.layers.29.per_layer_projection.weight', 'language_model.layers.29.post_attention_layernorm.weight', 'language_model.layers.29.post_feedforward_layernorm.weight', 'language_model.layers.29.post_per_layer_input_norm.weight', 'language_model.layers.29.pre_feedforward_layernorm.weight', 'language_model.layers.29.self_attn.k_norm.weight', 'language_model.layers.29.self_attn.k_proj.weight', 'language_model.layers.29.self_attn.o_proj.weight', 'language_model.layers.29.self_attn.q_norm.weight', 'language_model.layers.29.self_attn.q_proj.weight', 'language_model.layers.29.self_attn.v_proj.weight', 'language_model.layers.3.altup.correct_output_scale', 'language_model.layers.3.altup.correction_coefs.weight', 'language_model.layers.3.altup.modality_router.weight', 'language_model.layers.3.altup.prediction_coefs.weight', 'language_model.layers.3.altup.router_norm.weight', 'language_model.layers.3.input_layernorm.weight', 'language_model.layers.3.laurel.linear_left.weight', 'language_model.layers.3.laurel.linear_right.weight', 'language_model.layers.3.laurel.post_laurel_norm.weight', 'language_model.layers.3.mlp.down_proj.weight', 'language_model.layers.3.mlp.gate_proj.weight', 'language_model.layers.3.mlp.up_proj.weight', 'language_model.layers.3.per_layer_input_gate.weight', 'language_model.layers.3.per_layer_projection.weight', 'language_model.layers.3.post_attention_layernorm.weight', 'language_model.layers.3.post_feedforward_layernorm.weight', 'language_model.layers.3.post_per_layer_input_norm.weight', 'language_model.layers.3.pre_feedforward_layernorm.weight', 'language_model.layers.3.self_attn.k_norm.weight', 'language_model.layers.3.self_attn.k_proj.weight', 'language_model.layers.3.self_attn.o_proj.weight', 'language_model.layers.3.self_attn.q_norm.weight', 'language_model.layers.3.self_attn.q_proj.weight', 'language_model.layers.3.self_attn.v_proj.weight', 'language_model.layers.4.altup.correct_output_scale', 'language_model.layers.4.altup.correction_coefs.weight', 'language_model.layers.4.altup.modality_router.weight', 'language_model.layers.4.altup.prediction_coefs.weight', 'language_model.layers.4.altup.router_norm.weight', 'language_model.layers.4.input_layernorm.weight', 'language_model.layers.4.laurel.linear_left.weight', 'language_model.layers.4.laurel.linear_right.weight', 'language_model.layers.4.laurel.post_laurel_norm.weight', 'language_model.layers.4.mlp.down_proj.weight', 'language_model.layers.4.mlp.gate_proj.weight', 'language_model.layers.4.mlp.up_proj.weight', 'language_model.layers.4.per_layer_input_gate.weight', 'language_model.layers.4.per_layer_projection.weight', 'language_model.layers.4.post_attention_layernorm.weight', 'language_model.layers.4.post_feedforward_layernorm.weight', 'language_model.layers.4.post_per_layer_input_norm.weight', 'language_model.layers.4.pre_feedforward_layernorm.weight', 'language_model.layers.4.self_attn.k_norm.weight', 'language_model.layers.4.self_attn.k_proj.weight', 'language_model.layers.4.self_attn.o_proj.weight', 'language_model.layers.4.self_attn.q_norm.weight', 'language_model.layers.4.self_attn.q_proj.weight', 'language_model.layers.4.self_attn.v_proj.weight', 'language_model.layers.5.altup.correct_output_scale', 'language_model.layers.5.altup.correction_coefs.weight', 'language_model.layers.5.altup.modality_router.weight', 'language_model.layers.5.altup.prediction_coefs.weight', 'language_model.layers.5.altup.router_norm.weight', 'language_model.layers.5.input_layernorm.weight', 'language_model.layers.5.laurel.linear_left.weight', 'language_model.layers.5.laurel.linear_right.weight', 'language_model.layers.5.laurel.post_laurel_norm.weight', 'language_model.layers.5.mlp.down_proj.weight', 'language_model.layers.5.mlp.gate_proj.weight', 'language_model.layers.5.mlp.up_proj.weight', 'language_model.layers.5.per_layer_input_gate.weight', 'language_model.layers.5.per_layer_projection.weight', 'language_model.layers.5.post_attention_layernorm.weight', 'language_model.layers.5.post_feedforward_layernorm.weight', 'language_model.layers.5.post_per_layer_input_norm.weight', 'language_model.layers.5.pre_feedforward_layernorm.weight', 'language_model.layers.5.self_attn.k_norm.weight', 'language_model.layers.5.self_attn.k_proj.weight', 'language_model.layers.5.self_attn.o_proj.weight', 'language_model.layers.5.self_attn.q_norm.weight', 'language_model.layers.5.self_attn.q_proj.weight', 'language_model.layers.5.self_attn.v_proj.weight', 'language_model.layers.6.altup.correct_output_scale', 'language_model.layers.6.altup.correction_coefs.weight', 'language_model.layers.6.altup.modality_router.weight', 'language_model.layers.6.altup.prediction_coefs.weight', 'language_model.layers.6.altup.router_norm.weight', 'language_model.layers.6.input_layernorm.weight', 'language_model.layers.6.laurel.linear_left.weight', 'language_model.layers.6.laurel.linear_right.weight', 'language_model.layers.6.laurel.post_laurel_norm.weight', 'language_model.layers.6.mlp.down_proj.weight', 'language_model.layers.6.mlp.gate_proj.weight', 'language_model.layers.6.mlp.up_proj.weight', 'language_model.layers.6.per_layer_input_gate.weight', 'language_model.layers.6.per_layer_projection.weight', 'language_model.layers.6.post_attention_layernorm.weight', 'language_model.layers.6.post_feedforward_layernorm.weight', 'language_model.layers.6.post_per_layer_input_norm.weight', 'language_model.layers.6.pre_feedforward_layernorm.weight', 'language_model.layers.6.self_attn.k_norm.weight', 'language_model.layers.6.self_attn.k_proj.weight', 'language_model.layers.6.self_attn.o_proj.weight', 'language_model.layers.6.self_attn.q_norm.weight', 'language_model.layers.6.self_attn.q_proj.weight', 'language_model.layers.6.self_attn.v_proj.weight', 'language_model.layers.7.altup.correct_output_scale', 'language_model.layers.7.altup.correction_coefs.weight', 'language_model.layers.7.altup.modality_router.weight', 'language_model.layers.7.altup.prediction_coefs.weight', 'language_model.layers.7.altup.router_norm.weight', 'language_model.layers.7.input_layernorm.weight', 'language_model.layers.7.laurel.linear_left.weight', 'language_model.layers.7.laurel.linear_right.weight', 'language_model.layers.7.laurel.post_laurel_norm.weight', 'language_model.layers.7.mlp.down_proj.weight', 'language_model.layers.7.mlp.gate_proj.weight', 'language_model.layers.7.mlp.up_proj.weight', 'language_model.layers.7.per_layer_input_gate.weight', 'language_model.layers.7.per_layer_projection.weight', 'language_model.layers.7.post_attention_layernorm.weight', 'language_model.layers.7.post_feedforward_layernorm.weight', 'language_model.layers.7.post_per_layer_input_norm.weight', 'language_model.layers.7.pre_feedforward_layernorm.weight', 'language_model.layers.7.self_attn.k_norm.weight', 'language_model.layers.7.self_attn.k_proj.weight', 'language_model.layers.7.self_attn.o_proj.weight', 'language_model.layers.7.self_attn.q_norm.weight', 'language_model.layers.7.self_attn.q_proj.weight', 'language_model.layers.7.self_attn.v_proj.weight', 'language_model.layers.8.altup.correct_output_scale', 'language_model.layers.8.altup.correction_coefs.weight', 'language_model.layers.8.altup.modality_router.weight', 'language_model.layers.8.altup.prediction_coefs.weight', 'language_model.layers.8.altup.router_norm.weight', 'language_model.layers.8.input_layernorm.weight', 'language_model.layers.8.laurel.linear_left.weight', 'language_model.layers.8.laurel.linear_right.weight', 'language_model.layers.8.laurel.post_laurel_norm.weight', 'language_model.layers.8.mlp.down_proj.weight', 'language_model.layers.8.mlp.gate_proj.weight', 'language_model.layers.8.mlp.up_proj.weight', 'language_model.layers.8.per_layer_input_gate.weight', 'language_model.layers.8.per_layer_projection.weight', 'language_model.layers.8.post_attention_layernorm.weight', 'language_model.layers.8.post_feedforward_layernorm.weight', 'language_model.layers.8.post_per_layer_input_norm.weight', 'language_model.layers.8.pre_feedforward_layernorm.weight', 'language_model.layers.8.self_attn.k_norm.weight', 'language_model.layers.8.self_attn.k_proj.weight', 'language_model.layers.8.self_attn.o_proj.weight', 'language_model.layers.8.self_attn.q_norm.weight', 'language_model.layers.8.self_attn.q_proj.weight', 'language_model.layers.8.self_attn.v_proj.weight', 'language_model.layers.9.altup.correct_output_scale', 'language_model.layers.9.altup.correction_coefs.weight', 'language_model.layers.9.altup.modality_router.weight', 'language_model.layers.9.altup.prediction_coefs.weight', 'language_model.layers.9.altup.router_norm.weight', 'language_model.layers.9.input_layernorm.weight', 'language_model.layers.9.laurel.linear_left.weight', 'language_model.layers.9.laurel.linear_right.weight', 'language_model.layers.9.laurel.post_laurel_norm.weight', 'language_model.layers.9.mlp.down_proj.weight', 'language_model.layers.9.mlp.gate_proj.weight', 'language_model.layers.9.mlp.up_proj.weight', 'language_model.layers.9.per_layer_input_gate.weight', 'language_model.layers.9.per_layer_projection.weight', 'language_model.layers.9.post_attention_layernorm.weight', 'language_model.layers.9.post_feedforward_layernorm.weight', 'language_model.layers.9.post_per_layer_input_norm.weight', 'language_model.layers.9.pre_feedforward_layernorm.weight', 'language_model.layers.9.self_attn.k_norm.weight', 'language_model.layers.9.self_attn.k_proj.weight', 'language_model.layers.9.self_attn.o_proj.weight', 'language_model.layers.9.self_attn.q_norm.weight', 'language_model.layers.9.self_attn.q_proj.weight', 'language_model.layers.9.self_attn.v_proj.weight', 'language_model.norm.weight', 'language_model.per_layer_model_projection.weight', 'language_model.per_layer_projection_norm.weight', 'vision_tower.timm_model.blocks.0.0.bn1.weight', 'vision_tower.timm_model.blocks.0.0.bn2.weight', 'vision_tower.timm_model.blocks.0.0.conv_exp.weight', 'vision_tower.timm_model.blocks.0.0.conv_pwl.weight', 'vision_tower.timm_model.blocks.0.1.bn1.weight', 'vision_tower.timm_model.blocks.0.1.bn2.weight', 'vision_tower.timm_model.blocks.0.1.conv_exp.weight', 'vision_tower.timm_model.blocks.0.1.conv_pwl.weight', 'vision_tower.timm_model.blocks.0.2.bn1.weight', 'vision_tower.timm_model.blocks.0.2.bn2.weight', 'vision_tower.timm_model.blocks.0.2.conv_exp.weight', 'vision_tower.timm_model.blocks.0.2.conv_pwl.weight', 'vision_tower.timm_model.blocks.1.0.dw_mid.bn.weight', 'vision_tower.timm_model.blocks.1.0.dw_mid.conv.weight', 'vision_tower.timm_model.blocks.1.0.dw_start.bn.weight', 'vision_tower.timm_model.blocks.1.0.dw_start.conv.weight', 'vision_tower.timm_model.blocks.1.0.layer_scale.gamma', 'vision_tower.timm_model.blocks.1.0.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.1.0.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.1.0.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.1.0.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.1.1.dw_start.bn.weight', 'vision_tower.timm_model.blocks.1.1.dw_start.conv.weight', 'vision_tower.timm_model.blocks.1.1.layer_scale.gamma', 'vision_tower.timm_model.blocks.1.1.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.1.1.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.1.1.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.1.1.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.1.2.dw_start.bn.weight', 'vision_tower.timm_model.blocks.1.2.dw_start.conv.weight', 'vision_tower.timm_model.blocks.1.2.layer_scale.gamma', 'vision_tower.timm_model.blocks.1.2.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.1.2.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.1.2.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.1.2.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.1.3.dw_start.bn.weight', 'vision_tower.timm_model.blocks.1.3.dw_start.conv.weight', 'vision_tower.timm_model.blocks.1.3.layer_scale.gamma', 'vision_tower.timm_model.blocks.1.3.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.1.3.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.1.3.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.1.3.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.1.4.dw_start.bn.weight', 'vision_tower.timm_model.blocks.1.4.dw_start.conv.weight', 'vision_tower.timm_model.blocks.1.4.layer_scale.gamma', 'vision_tower.timm_model.blocks.1.4.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.1.4.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.1.4.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.1.4.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.0.dw_mid.bn.weight', 'vision_tower.timm_model.blocks.2.0.dw_mid.conv.weight', 'vision_tower.timm_model.blocks.2.0.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.0.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.0.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.0.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.0.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.0.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.0.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.1.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.1.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.1.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.1.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.1.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.1.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.1.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.10.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.10.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.10.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.10.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.10.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.11.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.11.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.11.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.11.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.11.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.11.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.11.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.11.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.11.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.11.norm.weight', 'vision_tower.timm_model.blocks.2.12.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.12.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.12.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.12.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.12.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.13.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.13.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.13.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.13.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.13.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.13.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.13.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.13.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.13.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.13.norm.weight', 'vision_tower.timm_model.blocks.2.14.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.14.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.14.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.14.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.14.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.15.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.15.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.15.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.15.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.15.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.15.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.15.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.15.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.15.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.15.norm.weight', 'vision_tower.timm_model.blocks.2.16.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.16.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.16.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.16.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.16.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.17.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.17.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.17.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.17.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.17.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.17.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.17.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.17.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.17.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.17.norm.weight', 'vision_tower.timm_model.blocks.2.18.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.18.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.18.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.18.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.18.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.19.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.19.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.19.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.19.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.19.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.19.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.19.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.19.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.19.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.19.norm.weight', 'vision_tower.timm_model.blocks.2.2.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.2.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.2.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.2.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.2.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.2.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.2.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.20.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.20.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.20.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.20.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.20.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.21.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.21.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.21.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.21.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.21.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.21.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.21.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.21.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.21.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.21.norm.weight', 'vision_tower.timm_model.blocks.2.22.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.22.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.22.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.22.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.22.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.23.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.23.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.23.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.23.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.23.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.23.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.23.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.23.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.23.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.23.norm.weight', 'vision_tower.timm_model.blocks.2.24.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.24.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.24.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.24.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.24.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.25.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.25.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.25.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.25.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.25.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.25.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.25.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.25.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.25.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.25.norm.weight', 'vision_tower.timm_model.blocks.2.26.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.26.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.26.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.26.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.26.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.27.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.27.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.27.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.27.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.27.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.27.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.27.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.27.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.27.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.27.norm.weight', 'vision_tower.timm_model.blocks.2.28.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.28.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.28.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.28.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.28.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.29.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.29.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.29.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.29.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.29.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.29.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.29.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.29.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.29.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.29.norm.weight', 'vision_tower.timm_model.blocks.2.3.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.3.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.3.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.3.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.3.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.3.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.3.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.30.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.30.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.30.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.30.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.30.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.31.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.31.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.31.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.31.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.31.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.31.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.31.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.31.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.31.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.31.norm.weight', 'vision_tower.timm_model.blocks.2.32.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.32.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.32.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.32.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.32.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.33.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.33.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.33.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.33.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.33.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.33.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.33.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.33.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.33.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.33.norm.weight', 'vision_tower.timm_model.blocks.2.34.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.34.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.34.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.34.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.34.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.35.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.35.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.35.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.35.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.35.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.35.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.35.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.35.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.35.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.35.norm.weight', 'vision_tower.timm_model.blocks.2.36.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.36.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.36.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.36.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.36.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.4.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.4.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.4.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.4.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.4.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.4.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.4.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.5.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.5.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.5.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.5.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.5.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.5.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.5.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.6.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.6.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.6.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.6.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.6.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.6.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.6.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.7.dw_start.bn.weight', 'vision_tower.timm_model.blocks.2.7.dw_start.conv.weight', 'vision_tower.timm_model.blocks.2.7.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.7.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.7.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.7.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.7.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.8.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.8.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.2.8.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.2.8.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.2.8.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.2.9.attn.key.down_conv.weight', 'vision_tower.timm_model.blocks.2.9.attn.key.norm.weight', 'vision_tower.timm_model.blocks.2.9.attn.key.proj.weight', 'vision_tower.timm_model.blocks.2.9.attn.output.proj.weight', 'vision_tower.timm_model.blocks.2.9.attn.query.proj.weight', 'vision_tower.timm_model.blocks.2.9.attn.value.down_conv.weight', 'vision_tower.timm_model.blocks.2.9.attn.value.norm.weight', 'vision_tower.timm_model.blocks.2.9.attn.value.proj.weight', 'vision_tower.timm_model.blocks.2.9.layer_scale.gamma', 'vision_tower.timm_model.blocks.2.9.norm.weight', 'vision_tower.timm_model.blocks.3.0.dw_mid.bn.weight', 'vision_tower.timm_model.blocks.3.0.dw_mid.conv.weight', 'vision_tower.timm_model.blocks.3.0.dw_start.bn.weight', 'vision_tower.timm_model.blocks.3.0.dw_start.conv.weight', 'vision_tower.timm_model.blocks.3.0.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.0.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.0.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.0.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.0.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.1.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.1.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.1.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.1.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.1.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.1.norm.weight', 'vision_tower.timm_model.blocks.3.10.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.10.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.10.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.10.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.10.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.11.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.11.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.11.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.11.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.11.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.11.norm.weight', 'vision_tower.timm_model.blocks.3.12.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.12.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.12.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.12.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.12.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.13.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.13.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.13.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.13.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.13.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.13.norm.weight', 'vision_tower.timm_model.blocks.3.14.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.14.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.14.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.14.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.14.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.15.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.15.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.15.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.15.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.15.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.15.norm.weight', 'vision_tower.timm_model.blocks.3.16.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.16.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.16.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.16.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.16.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.17.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.17.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.17.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.17.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.17.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.17.norm.weight', 'vision_tower.timm_model.blocks.3.18.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.18.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.18.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.18.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.18.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.19.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.19.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.19.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.19.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.19.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.19.norm.weight', 'vision_tower.timm_model.blocks.3.2.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.2.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.2.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.2.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.2.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.20.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.20.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.20.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.20.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.20.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.21.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.21.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.21.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.21.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.21.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.21.norm.weight', 'vision_tower.timm_model.blocks.3.22.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.22.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.22.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.22.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.22.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.23.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.23.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.23.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.23.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.23.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.23.norm.weight', 'vision_tower.timm_model.blocks.3.24.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.24.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.24.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.24.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.24.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.25.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.25.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.25.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.25.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.25.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.25.norm.weight', 'vision_tower.timm_model.blocks.3.26.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.26.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.26.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.26.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.26.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.27.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.27.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.27.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.27.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.27.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.27.norm.weight', 'vision_tower.timm_model.blocks.3.28.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.28.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.28.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.28.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.28.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.29.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.29.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.29.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.29.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.29.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.29.norm.weight', 'vision_tower.timm_model.blocks.3.3.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.3.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.3.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.3.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.3.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.3.norm.weight', 'vision_tower.timm_model.blocks.3.30.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.30.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.30.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.30.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.30.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.31.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.31.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.31.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.31.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.31.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.31.norm.weight', 'vision_tower.timm_model.blocks.3.32.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.32.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.32.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.32.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.32.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.33.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.33.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.33.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.33.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.33.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.33.norm.weight', 'vision_tower.timm_model.blocks.3.34.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.34.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.34.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.34.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.34.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.35.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.35.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.35.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.35.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.35.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.35.norm.weight', 'vision_tower.timm_model.blocks.3.36.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.36.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.36.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.36.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.36.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.37.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.37.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.37.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.37.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.37.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.37.norm.weight', 'vision_tower.timm_model.blocks.3.38.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.38.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.38.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.38.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.38.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.4.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.4.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.4.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.4.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.4.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.5.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.5.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.5.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.5.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.5.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.5.norm.weight', 'vision_tower.timm_model.blocks.3.6.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.6.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.6.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.6.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.6.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.7.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.7.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.7.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.7.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.7.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.7.norm.weight', 'vision_tower.timm_model.blocks.3.8.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.8.pw_exp.bn.weight', 'vision_tower.timm_model.blocks.3.8.pw_exp.conv.weight', 'vision_tower.timm_model.blocks.3.8.pw_proj.bn.weight', 'vision_tower.timm_model.blocks.3.8.pw_proj.conv.weight', 'vision_tower.timm_model.blocks.3.9.attn.key.proj.weight', 'vision_tower.timm_model.blocks.3.9.attn.output.proj.weight', 'vision_tower.timm_model.blocks.3.9.attn.query.proj.weight', 'vision_tower.timm_model.blocks.3.9.attn.value.proj.weight', 'vision_tower.timm_model.blocks.3.9.layer_scale.gamma', 'vision_tower.timm_model.blocks.3.9.norm.weight', 'vision_tower.timm_model.conv_stem.bn.weight', 'vision_tower.timm_model.conv_stem.conv.bias', 'vision_tower.timm_model.conv_stem.conv.weight', 'vision_tower.timm_model.msfa.ffn.pw_exp.bn.weight', 'vision_tower.timm_model.msfa.ffn.pw_exp.conv.weight', 'vision_tower.timm_model.msfa.ffn.pw_proj.bn.weight', 'vision_tower.timm_model.msfa.ffn.pw_proj.conv.weight', 'vision_tower.timm_model.msfa.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "# 1) Load Gemma 3n (pick an effective size that fits your hardware)\n",
    "MODEL_ID = \"google/gemma-3n-E2B\"   # also try: \"google/gemma-3n-E4B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "# model = AutoModel.from_pretrained(MODEL_ID).to(device)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "print(\"Loading quantized model… (this can take a moment)\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_4bit,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=attn_impl,  # uncomment if flash‑attn installed\n",
    ")\n",
    "model.eval()\n",
    "print(\"Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e31c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Load an audio file (mono float32). Gemma 3n expects ~16 kHz; we’ll resample if needed.\n",
    "# Replace \"example.wav\" with your file path, or load arrays directly.\n",
    "\n",
    "path = '/mnt/c/Users/user/Desktop/Roshidat/Workspace/PD_prediction/data/1_data/HC_AH/AH_064F_7AB034C9-72E4-438B-A9B3-AD7FDA1596C5.wav'\n",
    "wav, sr = sf.read(path)\n",
    "if wav.ndim > 1:\n",
    "    wav = librosa.to_mono(wav.T)\n",
    "target_sr = getattr(processor.feature_extractor, \"sampling_rate\", 16000)\n",
    "if sr != target_sr:\n",
    "    wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)\n",
    "    sr = target_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ba37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav type: <class 'numpy.ndarray'> shape: (59822,)\n",
      "sr: 16000\n",
      "wav after checks: <class 'numpy.ndarray'> (59822,) float32\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m     wav = wav.astype(np.float32)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mwav after checks:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(wav), wav.shape, wav.dtype)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/home/lib/python3.12/site-packages/transformers/models/gemma3n/processing_gemma3n.py:122\u001b[39m, in \u001b[36mGemma3nProcessor.__call__\u001b[39m\u001b[34m(self, images, text, audio, videos, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    121\u001b[39m     text = [text]\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid input text. Please provide a string, or a list of strings\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('wav type:', type(wav), 'shape:', getattr(wav, 'shape', None))\n",
    "print('sr:', sr)\n",
    "if wav is None:\n",
    "    raise ValueError('wav is None')\n",
    "if not isinstance(wav, np.ndarray):\n",
    "    wav = np.asarray(wav, dtype=np.float32)\n",
    "if wav.ndim > 1:\n",
    "    wav = librosa.to_mono(wav.T)\n",
    "if wav.dtype != np.float32:\n",
    "    wav = wav.astype(np.float32)\n",
    "print('wav after checks:', type(wav), wav.shape, wav.dtype)\n",
    "inputs = processor(audio=wav, sampling_rate=sr, return_tensors=\"pt\")\n",
    "if inputs is None:\n",
    "    raise ValueError('processor returned None. Check audio format and processor compatibility.')\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78447568",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53840b12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3) Prepare inputs for the audio encoder\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# AutoProcessor will produce `input_features` (Mel-spec frames) and a mask.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 4) Forward pass to get audio hidden states\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Set output_hidden_states=True so we can choose which representation to pool.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/home/lib/python3.12/site-packages/transformers/models/gemma3n/processing_gemma3n.py:122\u001b[39m, in \u001b[36mGemma3nProcessor.__call__\u001b[39m\u001b[34m(self, images, text, audio, videos, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    121\u001b[39m     text = [text]\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid input text. Please provide a string, or a list of strings\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# # 3) Prepare inputs for the audio encoder\n",
    "# # AutoProcessor will produce `input_features` (Mel-spec frames) and a mask.\n",
    "# inputs = processor(audio=wav, sampling_rate=sr, return_tensors=\"pt\")\n",
    "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# # 4) Forward pass to get audio hidden states\n",
    "# # Set output_hidden_states=True so we can choose which representation to pool.\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efbe249",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3) Prepare inputs for the audio encoder\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# AutoProcessor will produce `input_features` (Mel-spec frames) and a mask.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 4) Forward pass to get audio hidden states\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Set output_hidden_states=True so we can choose which representation to pool.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/home/lib/python3.12/site-packages/transformers/models/gemma3n/processing_gemma3n.py:122\u001b[39m, in \u001b[36mGemma3nProcessor.__call__\u001b[39m\u001b[34m(self, images, text, audio, videos, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    121\u001b[39m     text = [text]\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid input text. Please provide a string, or a list of strings\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 5) Grab the last audio-layer features and mean-pool over the time dimension\n",
    "# `audio_hidden_states` has shape: (batch, seq_len, hidden_size)\n",
    "audio_hs = outputs.audio_hidden_states  # tuple of layer outputs\n",
    "last_audio = audio_hs[-1]               # (B, T, D)\n",
    "\n",
    "# If a frame mask was returned, use it for masked pooling (robust to padding)\n",
    "mask = inputs.get(\"input_features_mask\")  # (B, T)\n",
    "if mask is not None:\n",
    "    mask = mask.to(last_audio.dtype).unsqueeze(-1)  # (B, T, 1)\n",
    "    summed = (last_audio * mask).sum(dim=1)         # (B, D)\n",
    "    lengths = mask.sum(dim=1).clamp(min=1e-6)       # (B, 1)\n",
    "    pooled = summed / lengths                        # (B, D)\n",
    "else:\n",
    "    pooled = last_audio.mean(dim=1)                 # (B, D)\n",
    "\n",
    "# `pooled` is your fixed-size embedding for downstream tasks.\n",
    "print(\"Embedding shape:\", pooled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ac4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 6) (Optional) Tiny classifier head example\n",
    "num_classes = 8  # change for your task\n",
    "clf = nn.Linear(pooled.shape[-1], num_classes).to(device)\n",
    "\n",
    "# Dummy forward (replace with your dataloader + training loop)\n",
    "logits = clf(pooled)            # (B, num_classes)\n",
    "probs = logits.softmax(dim=-1)  # class probabilities\n",
    "print(\"Probs:\", probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
