{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2936a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292ffdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files.\n"
     ]
    }
   ],
   "source": [
    "Model_name = \"bert-base-uncased\"\n",
    "text_dir = \"/home/jovyan/Desktop/PD_LLM/data/voice_parkinson/HC_AH/HC_AH_TEXT_output\"\n",
    "out_Dir = os.path.join(text_dir, \"berts_feats\")\n",
    "max_length = 512\n",
    "batch_size = 16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "save_tokens = False\n",
    "\n",
    "os.makedirs(out_Dir, exist_ok=True)\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(text_dir, \"*.txt\")))\n",
    "print(f\"Found {len(files)} text files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3282aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-08-25T07:50:51.342027Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://cas-server.xethub.hf.co/reconstruction/44172b48ce7a9d951000a9d76ae331b155df32c9fd4e93239a233d720f471725\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: No address associated with hostname\" })) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:213\n",
      "\n",
      "  \u001b[2m2025-08-25T07:50:51.342114Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 1.178943499s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-08-25T07:50:52.562076Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://cas-server.xethub.hf.co/reconstruction/44172b48ce7a9d951000a9d76ae331b155df32c9fd4e93239a233d720f471725\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: No address associated with hostname\" })) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:213\n",
      "\n",
      "  \u001b[2m2025-08-25T07:50:52.562154Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #1. Sleeping 2.949901728s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-08-25T07:50:55.514717Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://cas-server.xethub.hf.co/reconstruction/44172b48ce7a9d951000a9d76ae331b155df32c9fd4e93239a233d720f471725\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: No address associated with hostname\" })) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:213\n",
      "\n",
      "  \u001b[2m2025-08-25T07:50:55.514792Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #2. Sleeping 1.252213342s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-08-25T07:50:56.769328Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://cas-server.xethub.hf.co/reconstruction/44172b48ce7a9d951000a9d76ae331b155df32c9fd4e93239a233d720f471725\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: No address associated with hostname\" })) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:213\n",
      "\n",
      "  \u001b[2m2025-08-25T07:50:56.769371Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #3. Sleeping 9.92743919s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Desktop/PD_LLM/data/voice_parkinson/HC_AH/HC_AH_TEXT_output/berts_feats/cls_vectors.npy (41, 768)\n",
      "/home/jovyan/Desktop/PD_LLM/data/voice_parkinson/HC_AH/HC_AH_TEXT_output/berts_feats/mean_vectors.npy (41, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Model_name, token=None)\n",
    "model = AutoModel.from_pretrained(Model_name, token=None)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def read_texts(paths):\n",
    "    texts = []\n",
    "    for p in paths:\n",
    "        with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "cls_vectors = []\n",
    "mean_vectors = []\n",
    "\n",
    "for i in tqdm(range(0, len(files), batch_size)):\n",
    "    batch_paths = files[i:i + batch_size]\n",
    "    texts = read_texts(batch_paths)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length = max_length,\n",
    "    )\n",
    "\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc)\n",
    "      \n",
    "        last_hidden_state = outputs.last_hidden_state  # (B, T, D)\n",
    "\n",
    "        cls = last_hidden_state[:, 0, :]\n",
    "\n",
    "        mask = enc[\"attention_mask\"].unsqueeze(-1)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        counts = mask.sum(dim=1).clamp(min=1)\n",
    "        mean = summed / counts\n",
    "        \n",
    "        cls_vectors.append(cls.cpu().numpy())\n",
    "        mean_vectors.append(mean.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        if save_tokens:\n",
    "            \n",
    "            for j, p in enumerate(batch_paths):\n",
    "                L = int(enc[\"attention_mask\"][j].sum().item())\n",
    "                tokenizer_features = last_hidden_state[j, :L, :].cpu().numpy()\n",
    "                base = os.path.splitext(os.path.basename(p))[0]\n",
    "                np.save(os.path.join(out_Dir, f\"{base}.tokens.last.npy\"), tokenizer_features)\n",
    "\n",
    "cls_vectors = np.vstack(cls_vectors)\n",
    "mean_vectors = np.vstack(mean_vectors)\n",
    "np.save(os.path.join(out_Dir, \"cls_vectors.npy\"), cls_vectors)\n",
    "np.save(os.path.join(out_Dir, \"mean_vectors.npy\"), mean_vectors)\n",
    "\n",
    "\n",
    "print(os.path.join(out_Dir, \"cls_vectors.npy\"), cls_vectors.shape)\n",
    "print(os.path.join(out_Dir, \"mean_vectors.npy\"), mean_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58788d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 text files.\n"
     ]
    }
   ],
   "source": [
    "pd_text_dir = \"/home/jovyan/Desktop/PD_LLM/data/voice_parkinson/PD_AH/PD_AH_Text_output\"\n",
    "pd_out_Dir = os.path.join(pd_text_dir, \"berts_feats\")\n",
    "max_length = 512\n",
    "batch_size = 16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "save_tokens = False\n",
    "\n",
    "os.makedirs(pd_out_Dir, exist_ok=True)\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(pd_text_dir, \"*.txt\")))\n",
    "print(f\"Found {len(files)} text files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a85f3186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 43.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Desktop/PD_LLM/data/voice_parkinson/HC_AH/HC_AH_TEXT_output/berts_feats/cls_vectors.npy (40, 768)\n",
      "/home/jovyan/Desktop/PD_LLM/data/voice_parkinson/HC_AH/HC_AH_TEXT_output/berts_feats/mean_vectors.npy (40, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Model_name, token=None)\n",
    "model = AutoModel.from_pretrained(Model_name, token=None)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def read_texts(paths):\n",
    "    texts = []\n",
    "    for p in paths:\n",
    "        with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "cls_vectors = []\n",
    "mean_vectors = []\n",
    "\n",
    "for i in tqdm(range(0, len(files), batch_size)):\n",
    "    batch_paths = files[i:i + batch_size]\n",
    "    texts = read_texts(batch_paths)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length = max_length,\n",
    "    )\n",
    "\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc)\n",
    "      \n",
    "        last_hidden_state = outputs.last_hidden_state  # (B, T, D)\n",
    "\n",
    "        cls = last_hidden_state[:, 0, :]\n",
    "\n",
    "        mask = enc[\"attention_mask\"].unsqueeze(-1)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        counts = mask.sum(dim=1).clamp(min=1)\n",
    "        mean = summed / counts\n",
    "        \n",
    "        cls_vectors.append(cls.cpu().numpy())\n",
    "        mean_vectors.append(mean.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        if save_tokens:\n",
    "            \n",
    "            for j, p in enumerate(batch_paths):\n",
    "                L = int(enc[\"attention_mask\"][j].sum().item())\n",
    "                tokenizer_features = last_hidden_state[j, :L, :].cpu().numpy()\n",
    "                base = os.path.splitext(os.path.basename(p))[0]\n",
    "                np.save(os.path.join(out_Dir, f\"{base}.tokens.last.npy\"), tokenizer_features)\n",
    "\n",
    "cls_vectors = np.vstack(cls_vectors)\n",
    "mean_vectors = np.vstack(mean_vectors)\n",
    "np.save(os.path.join(out_Dir, \"cls_vectors.npy\"), cls_vectors)\n",
    "np.save(os.path.join(out_Dir, \"mean_vectors.npy\"), mean_vectors)\n",
    "\n",
    "\n",
    "print(os.path.join(out_Dir, \"cls_vectors.npy\"), cls_vectors.shape)\n",
    "print(os.path.join(out_Dir, \"mean_vectors.npy\"), mean_vectors.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
